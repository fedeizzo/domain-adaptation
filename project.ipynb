{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c5ff57-5d6c-4009-890a-908f45db27c0",
   "metadata": {},
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ebeb45-dc48-4dd6-93cf-8e8c26149403",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ZIP_PATH = '/tmp/Adaptiope.zip'\n",
    "DATASET_EXTRACTION_PATH = '/tmp/Adaptiope'\n",
    "DATASET_PATH = Path('./data/adaptiope_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5bab7e-f73f-4c6c-8604-ed6e14350173",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dataset extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699fc78-628c-453d-a8b0-0d995e3bbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join\n",
    "from shutil import copytree\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1adf6b-7d09-4598-b98f-aff561576ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {DATASET_EXTRACTION_PATH}\n",
    "!unzip -d {DATASET_EXTRACTION_PATH} {DATASET_ZIP_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3b655-0048-437b-909a-ae4c87356349",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
    "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
    "           \"purse\", \"stand mixer\", \"stroller\"]\n",
    "for d, td in zip([\n",
    "    f\"{DATASET_EXTRACTION_PATH}/Adaptiope/product_images\",\n",
    "    f\"{DATASET_EXTRACTION_PATH}/Adaptiope/real_life\"],[\n",
    "    f\"{DATASET_PATH}/product_images\",\n",
    "    f\"{DATASET_PATH}/real_life\"]):\n",
    "    makedirs(td)\n",
    "    for c in classes:\n",
    "        c_path = join(d, c)\n",
    "        c_target = join(td, c)\n",
    "        copytree(c_path, c_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc9ae4a-9559-4fa9-910d-4e5ecf2cd704",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b4c4e-d619-4fc4-bd65-5c4b2f938f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b44fb-0bfe-4383-89d0-9d1b80a3fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(DATASET_PATH / \"product_images\")\n",
    "idx_to_class = {v: k for k,v in dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59401c76-c3f8-4e6a-9c29-14bca1e48fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imgs: List[str, int] path, class\n",
    "seen_classes = set()\n",
    "imgs = []\n",
    "for i, (p, c) in enumerate(dataset.imgs):\n",
    "    if c not in seen_classes:\n",
    "        seen_classes.add(c)\n",
    "        imgs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531b13a5-f18f-433b-bc45-8f96d9718d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(10,10))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        image, title = dataset[imgs.pop(0)]\n",
    "        axs[i,j].imshow(image)\n",
    "        axs[i,j].set_title(idx_to_class[title])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffca8b-4272-48c6-8b99-436307356ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f32af-ad19-47cc-99ff-b2949ac6893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_random_seed(seed=0) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980930b-3e99-4b28-8ae8-4032d4d5e343",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d60a6-89e7-4856-8c20-5b5b76406cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes: int, feature_dimension: int, dropout_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.feature_extractor = models.resnet34(pretrained=True)\n",
    "        self.feature_extractor.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        out_feature_extractor = self.feature_extractor.fc.in_features\n",
    "\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.feature_extractor.fc = nn.Sequential(\n",
    "            nn.Linear(out_feature_extractor, feature_dimension),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dimension, feature_dimension // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dimension // 2, num_classes),\n",
    "        )\n",
    "\n",
    "        init_modules = [\n",
    "            self.feature_extractor.fc,\n",
    "        ]\n",
    "\n",
    "        for m in init_modules:\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.feature_extractor(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c964e6-3309-43d7-837f-33d073209236",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f532981-1a8a-4c13-9ea4-4dcadf33a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da21f6-033e-4414-bdfa-76ad2cdabfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageFolder(\n",
    "    DATASET_PATH / \"product_images\",\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            T.Resize(224),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "validation = ImageFolder(\n",
    "    DATASET_PATH / \"product_images\",\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            T.Resize(224),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "test = ImageFolder(\n",
    "    DATASET_PATH / \"product_images\",\n",
    "    transform=T.Compose(\n",
    "        [\n",
    "            T.Resize(224),\n",
    "            T.CenterCrop(224),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d53f5-9caf-4e19-927b-7f03dca4ade3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd44987-1246-4469-a77d-c8de1ff8ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler, SGD, Adam\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c183b7-9d55-4fb2-a966-1c6f45ddb0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "set_random_seed(33)\n",
    "device = get_device()\n",
    "num_threads = 16\n",
    "\n",
    "# train\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "lr = 0.0001\n",
    "weight_decay = 0\n",
    "scheduler_step_size = 5\n",
    "scheduler_gamma = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421845ad-3131-42e8-84fd-b110c6a8a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, num_threads):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_threads\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e8f57-e68b-44ee-86df-b5f88f306684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomModel(len(train.classes), 128, 0.2)\n",
    "optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=scheduler_step_size,\n",
    "    gamma=scheduler_gamma,\n",
    ")\n",
    "dataloaders = {\n",
    "    'train': get_data_loader(train, batch_size, num_threads),\n",
    "    'validation': get_data_loader(validation, batch_size, num_threads),\n",
    "    'test': get_data_loader(test, batch_size, num_threads)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc9c5b-c3f1-45fb-aa0a-b8a03561392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model\n",
    "best_loss = np.Inf\n",
    "with tqdm(total=num_epochs) as pbar:\n",
    "    for epoch in range(num_epochs):\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        phases_loss = {}\n",
    "        for phase, dataloader in dataloaders.items():\n",
    "            if phase == \"train\":\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            epoch_loss = 0.0\n",
    "            for index, (x, labels) in enumerate(dataloader):\n",
    "                x = x.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.autocast(device_type=device.type):\n",
    "                    with torch.set_grad_enabled(phase == \"train\"):\n",
    "                        predictions = model(x)\n",
    "                        loss = criterion(predictions, labels)\n",
    "                        epoch_loss += loss.item()\n",
    "\n",
    "                        if phase == \"train\":\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            scheduler.step()\n",
    "\n",
    "            epoch_loss /= len(dataloader)\n",
    "            phases_loss[phase] = epoch_loss\n",
    "            \n",
    "            if phase == \"validation\" and abs(epoch_loss) <= abs(best_loss):\n",
    "                best_model = model\n",
    "                best_loss = epoch_loss\n",
    "        pbar.set_postfix(phases_loss)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361623f-143e-408c-9f85-9d2f0ed5196d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb5590-78cb-4913-a497-8e7d40ee89f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
