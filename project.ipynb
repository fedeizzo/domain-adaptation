{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bc6b70",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185850f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATASET_ZIP_PATH = '/tmp/Adaptiope.zip'\n",
    "DATASET_EXTRACTION_PATH = '/tmp/Adaptiope'\n",
    "DATASET_PATH = Path('./data/adaptiope_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a99d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b451f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join, isdir\n",
    "from shutil import copytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c3037",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {DATASET_EXTRACTION_PATH}\n",
    "!unzip -d {DATASET_EXTRACTION_PATH} {DATASET_ZIP_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b8dda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if not isdir(DATASET_PATH):\n",
    "    classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
    "               \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
    "               \"purse\", \"stand mixer\", \"stroller\"]\n",
    "    for d, td in zip([\n",
    "        f\"{DATASET_EXTRACTION_PATH}/Adaptiope/product_images\",\n",
    "        f\"{DATASET_EXTRACTION_PATH}/Adaptiope/real_life\"],[\n",
    "        f\"{DATASET_PATH}/product_images\",\n",
    "        f\"{DATASET_PATH}/real_life\"]):\n",
    "        makedirs(td)\n",
    "        for c in classes:\n",
    "            c_path = join(d, c)\n",
    "            c_target = join(td, c)\n",
    "            copytree(c_path, c_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44aef08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f93276",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd1b5d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(DATASET_PATH / \"product_images\")\n",
    "idx_to_class = {v: k for k,v in dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452664d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imgs: List[str, int] path, class\n",
    "seen_classes = set()\n",
    "imgs = []\n",
    "for i, (p, c) in enumerate(dataset.imgs):\n",
    "    if c not in seen_classes:\n",
    "        seen_classes.add(c)\n",
    "        imgs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39292dd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(10,10))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        image, title = dataset[imgs.pop(0)]\n",
    "        axs[i,j].imshow(image)\n",
    "        axs[i,j].set_title(idx_to_class[title])\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d402fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c1bed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def set_random_seed(seed=0) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def print_epoch_chart(\n",
    "    train_losses,\n",
    "    val_losses,\n",
    "    train_acc,\n",
    "    val_acc,\n",
    "    epoch,\n",
    "    num_epochs\n",
    "):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(15,7))\n",
    "    x = np.arange(len(train_losses))\n",
    "    loss_ax.plot(x, train_losses, label='train')\n",
    "    loss_ax.plot(x, val_losses, label='validation')\n",
    "    loss_ax.set_title(\"Loss\")\n",
    "    loss_ax.set_ylim([0, 5])\n",
    "    loss_ax.legend()\n",
    "    acc_ax.plot(x, train_acc, label='train')\n",
    "    acc_ax.plot(x, val_acc, label='validation')\n",
    "    acc_ax.set_title(\"Accuracy\")\n",
    "    acc_ax.legend()\n",
    "    fig.suptitle(f\"Epoch {epoch+1} of {num_epochs}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a19051",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46672b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def coral(source_extracted_features, target_extracted_features, **kwargs):\n",
    "    source = source_extracted_features.double()\n",
    "    target = target_extracted_features.double()\n",
    "    \n",
    "    d = source.data.shape[1]\n",
    "    ns, nt = source.data.shape[0], target.data.shape[0]\n",
    "    # source covariance\n",
    "    xm = torch.mean(source, 0, keepdim=True) - source\n",
    "    xc = xm.t() @ xm / (ns - 1)\n",
    "\n",
    "    # target covariance\n",
    "    xmt = torch.mean(target, 0, keepdim=True) - target\n",
    "    xct = xmt.t() @ xmt / (nt - 1)\n",
    "\n",
    "    # frobenius norm between source and target\n",
    "    loss = torch.mul((xc - xct), (xc - xct))\n",
    "    loss = torch.sum(loss) / (4*d*d)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c4dc7-22c9-497f-b216-dcf35c2f2d26",
   "metadata": {},
   "source": [
    "## Contrastive Domain Discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519abf4-3682-4823-8af7-7119fe5265a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\\require{color}$\n",
    "*Contrative Domain Discrepancy (CDD)* is a measure used to quantify the dissimilarity between classes between source and destination target. The computation is made on features extracted from linear layers of the network after the pretrained backbone module.\n",
    "\n",
    "TODO: WRITE EQUATION FROM PAPER\n",
    "\n",
    "Let's suppose the two following table contains the output of the two linear layers for the source dataset with a batch size equal to 5 and, an output size equal to 5 for the first layer and to 3 for the latter.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "    \\hline\n",
    "    0.496257 & 0.768222 & 0.0884774 & 0.13203  & 0.307423 \\\\\n",
    "    \\hline\n",
    "    0.634079 & 0.490093 & 0.896445  & 0.455628 & 0.632306 \\\\\n",
    "    \\hline\n",
    "    0.348893 & 0.401717 & 0.0223258 & 0.168859 & 0.293888 \\\\\n",
    "    \\hline\n",
    "    0.518522 & 0.697668 & 0.800011  & 0.161029 & 0.282269 \\\\\n",
    "    \\hline\n",
    "    0.681609 & 0.915194 & 0.3971    & 0.874156 & 0.419408 \\\\\n",
    "    \\hline\n",
    "\\end{array}\\quad\\quad\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "    \\hline\n",
    "    0.552907 & 0.952738  & 0.0361648 \\\\\n",
    "    \\hline\n",
    "    0.185231 & 0.373417  & 0.3051    \\\\\n",
    "    \\hline\n",
    "    0.932    & 0.17591   & 0.269834  \\\\\n",
    "    \\hline\n",
    "    0.15068  & 0.0317195 & 0.20813   \\\\\n",
    "    \\hline\n",
    "    0.929799 & 0.723109  & 0.742336  \\\\\n",
    "    \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Same for the target dataset with a batch size equal to 4\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "    \\hline\n",
    "    0.526296 & 0.243658 & 0.584592 & 0.0331526 & 0.138717 \\\\\n",
    "    \\hline\n",
    "    0.242235 & 0.815469 & 0.793161 & 0.278252  & 0.481959 \\\\\n",
    "    \\hline\n",
    "    0.81978  & 0.997067 & 0.698441 & 0.567546  & 0.835243 \\\\\n",
    "    \\hline\n",
    "    0.205599 & 0.593172 & 0.112347 & 0.153457  & 0.241708 \\\\\n",
    "    \\hline\n",
    "\\end{array}\\quad\\quad\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    "0.726237 & 0.70108  & 0.203824 \\\\\n",
    "\\hline\n",
    "0.651054 & 0.774486 & 0.436891 \\\\\n",
    "\\hline\n",
    "0.519091 & 0.615852 & 0.810188 \\\\\n",
    "\\hline\n",
    "0.980097 & 0.114688 & 0.316765 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For each dataset it is also required a list describing the distribution of samples for each class, for source dataset $[2,2,1]$, for target dataset $[1, 1, 2]$.\n",
    "\n",
    "#### Distance computation\n",
    "There are three types of distance that can be computed:\n",
    "\n",
    "- distance source/source dataset\n",
    "- distance target/target dataset\n",
    "- distance source/target dataset\n",
    "\n",
    "All three distances are computed for each output of linear layer aggregating the different number of features summing them (`compute_pairwise_distance` function).\n",
    "\n",
    "Obtained result for distance source/target with shape equal to `[batch size source X batch size target]`:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "Layer 1& Layer2 \\\\\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " 0.560438 & 0.615181 & 0.997362 & 0.120472  \\\\\n",
    " \\hline\n",
    " 0.591715 & 0.324145 & 0.384421 & 1.0529    \\\\\n",
    " \\hline\n",
    " 0.415092 & 0.82409  & 1.48532  & 0.0682521 \\\\\n",
    " \\hline\n",
    " 0.28955  & 0.143876 & 0.66175  & 0.583425  \\\\\n",
    " \\hline\n",
    " 1.29631  & 0.718872 & 0.383529 & 0.962352  \\\\\n",
    " \\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " 0.121484 & 0.201988 & 0.713748 & 0.963555   \\\\\n",
    " \\hline\n",
    " 0.410307 & 0.395216 & 0.425351 & 0.698889   \\\\\n",
    " \\hline\n",
    " 0.3225   & 0.465132 & 0.656027 & 0.00826399 \\\\\n",
    " \\hline\n",
    " 0.779328 & 0.854408 & 0.839412 & 0.706618   \\\\\n",
    " \\hline\n",
    " 0.331919 & 0.173635 & 0.184789 & 0.553817   \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The output for source/source and target/target has respectively shape of `[batch size source X batch size source]` and `[batch size target X batch size target]`. Distances of the same dataset require an extra step that iterates over the diagonal extracting square submatrices with size length equal to the amount of samples for a specific class inside the batch size under consideration. Obtained result for one layer of source/source distance (given distribution $[2,2,1]$):\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "Class1&Class2&Class3  \\\\\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    " 0        & 0.543124 \\\\\n",
    " \\hline\n",
    " 0.543124 & 0        \\\\\n",
    "\\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    " 0       & 0.63506 \\\\\n",
    " \\hline\n",
    " 0.63506 & 0       \\\\\n",
    "\\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    " 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The matrix of distance source/target is averaged over classes submatrices (colors are used to show class groups) creating a new maen matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\text{Layer1 before the average}&\\text{Layer1 after}\\\\\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " \\color{red}{0.560438} & \\color{green}{0.615181} & \\color{orange}{0.99736} & \\color{orange}{0.120472}  \\\\\n",
    " \\hline\n",
    " \\color{red}{0.591715} & \\color{green}{0.324145} & \\color{orange}{0.384421} & \\color{orange}{1.0529}    \\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.415092 }& \\color{brown}{0.82409  }& \\color{olive}{1.48532  }& \\color{olive}{0.0682521} \\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.28955  }& \\color{brown}{0.143876 }& \\color{olive}{0.66175  }& \\color{olive}{0.583425 } \\\\\n",
    " \\hline\n",
    " \\color{pink}{1.29631  }& \\color{darkgrey}{0.718872 }& \\color{teal}{0.383529 }& \\color{teal}{0.962352 } \\\\\n",
    " \\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " \\color{red}{0.511834} & \\color{green}{0.476363} & \\color{orange}{0.607436} & \\color{orange}{0.607436} \\\\\n",
    " \\hline\n",
    " \\color{red}{0.511834} & \\color{green}{0.476363} & \\color{orange}{0.607436} & \\color{orange}{0.607436} \\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.432476 }& \\color{brown}{0.397005 }& \\color{olive}{0.567757 }& \\color{olive}{0.567757 }\\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.432476 }& \\color{brown}{0.397005 }& \\color{olive}{0.567757 }& \\color{olive}{0.567757 }\\\\\n",
    " \\hline\n",
    " \\color{pink}{0.280219 }& \\color{darkgrey}{0.307591 }& \\color{teal}{0.655509 }& \\color{teal}{0.655509 }\\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "TODO: HIGHLIGHT THE PART OF THE EQUATION THAT IS DONE WITH THIS PROCEDURE\n",
    "\n",
    "Same mean procedure is applied to source/source and target/target data.\n",
    "\n",
    "#### Kernel computation\n",
    "Distances and mean matrices are then used to compute a kernel for each layer, the list of results are summed up. The final matrix has a shape of `[number of samples X number of samples]` and expresses the correlation between samles belonging batch sizes (in the following table source/target table is presented):\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " 3.92554 & 3.90333 & 3.65386 \\\\\n",
    " \\hline\n",
    " 3.52906 & 3.25892 & 4.40912 \\\\\n",
    " \\hline\n",
    " 0.74786 & 2.26064 & 4.51405 \\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### Maximum Mean Discrepancy (MMD) computation\n",
    "Three kernels extracted from source/source, targe/target and source/target are composed together to obtaind the MMD):\n",
    "\n",
    "TODO: WRITE EQUATION\n",
    "\n",
    "The result is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "  8.30697 &  8.33474 & 4.42708 \\\\\n",
    "  \\hline\n",
    "  9.13727 &  9.64986 & 3.07151 \\\\\n",
    "  \\hline\n",
    " 18.5041  & 15.4785  & 7.26769 \\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where the diagonal represents the intra class discrepancy and the rest of the matrix the infra class discrepancy.\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\text{Intra class elements}&\\text{Infra class elements}\\\\\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "  8.30697 &  \\color{lightgrey}{8.33474} & \\color{lightgrey}{4.42708} \\\\\n",
    "  \\hline\n",
    "  \\color{lightgrey}{9.13727} &  9.64986 & \\color{lightgrey}{3.07151} \\\\\n",
    "  \\hline\n",
    "  \\color{lightgrey}{18.5041}  & \\color{lightgrey}{15.4785}  & 7.26769 \\\\\n",
    " \\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "  \\color{lightgrey}{8.30697} &  8.33474 & 4.42708 \\\\\n",
    "  \\hline\n",
    "  9.13727 &  \\color{lightgrey}{9.64986} & 3.07151 \\\\\n",
    "  \\hline\n",
    " 18.5041  & 15.4785  & \\color{lightgrey}{7.26769} \\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### CDD, intra class, infra class computation\n",
    "The intra and infra class measures can be computed averaging over vectors of the previous step. The CDD instead can be obtained applying the following formula:\n",
    "\n",
    "TODO: WRITE EQUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465f3b0-4d96-40c0-8e0a-0ceb7ceb5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "class ConstrativeDomainDiscrepancy():\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        kernel_orders: Tuple[int],\n",
    "        kernel_multipliers: Tuple[int],\n",
    "        num_classes: int,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_orders = kernel_orders\n",
    "        self.kernel_multipliers = kernel_multipliers\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        assert num_layers == len(kernel_orders) == len(kernel_multipliers), \"There must be one kernel order and multiplier for each layer\"\n",
    "    \n",
    "    def activate(\n",
    "        self,\n",
    "        source_layers_out: List[torch.Tensor], # shape: [num_layers X batch_size_source X output_of_layer(i)]\n",
    "        target_layers_out: List[torch.Tensor], # shape: [num_layers X batch_size_target X output_of_layer(i)]\n",
    "        samples_for_class_source: List[int], # shape: [num_classes]\n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "    ) -> Dict[str, float]:\n",
    "        assert len(source_layers_out) == len(target_layers_out) == self.num_layers, \"number of layers must be the same\"\n",
    "\n",
    "        layers_dist = []\n",
    "        layers_mean = []\n",
    "        for src, trg in zip(source_layers_out, target_layers_out):\n",
    "            layer_distances, layer_mean_distances = self.activate_layer(\n",
    "                src,\n",
    "                trg,\n",
    "                samples_for_class_source,\n",
    "                samples_for_class_target\n",
    "            )\n",
    "            layers_dist.append(layer_distances)\n",
    "            layers_mean.append(layer_mean_distances)\n",
    "            \n",
    "        for i in range(self.num_layers):\n",
    "            for c in range(len(samples_for_class_source)):\n",
    "                # shape goes from [num_classes] to [num_classes X 1 X 1]\n",
    "                layers_mean[i]['ss'][c] = layers_mean[i]['ss'][c].view(\n",
    "                    len(samples_for_class_source), 1, 1\n",
    "                )\n",
    "                layers_mean[i]['tt'][c] = layers_mean[i]['tt'][c].view(\n",
    "                    len(samples_for_class_source), 1, 1\n",
    "                )\n",
    "        \n",
    "        class_kernel_dist = self.classes_mean_kernel(\n",
    "            samples_for_class_source,\n",
    "            samples_for_class_target,\n",
    "            self.compute_cum_kernel_dist(\n",
    "                [i['st'] for i in layers_dist],\n",
    "                [i['st'] for i in layers_mean]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        cum_kernel_dist_ss = []\n",
    "        cum_kernel_dist_tt = []\n",
    "        for class_index in range(len(samples_for_class_source)):\n",
    "            cum_kernel_dist_ss.append(\n",
    "                torch.mean(\n",
    "                    self.compute_cum_kernel_dist(\n",
    "                        [i['ss'] for i in layers_dist],\n",
    "                        [i['ss'] for i in layers_mean],\n",
    "                        class_index\n",
    "                    ).view(len(samples_for_class_source), -1), dim=1\n",
    "                )\n",
    "            )\n",
    "            cum_kernel_dist_tt.append(\n",
    "                torch.mean(\n",
    "                    self.compute_cum_kernel_dist(\n",
    "                        [i['tt'] for i in layers_dist],\n",
    "                        [i['tt'] for i in layers_mean],\n",
    "                        class_index\n",
    "                    ).view(len(samples_for_class_source), -1), dim=1\n",
    "                )\n",
    "            )\n",
    "        kernel_dist_ss = torch.stack(cum_kernel_dist_ss, dim=0)\n",
    "        # understand why transpose here\n",
    "        kernel_dist_tt = torch.stack(cum_kernel_dist_tt, dim=0).transpose(1, 0)\n",
    "        \n",
    "        mmds = kernel_dist_ss + kernel_dist_tt - 2 * class_kernel_dist\n",
    "        \n",
    "        intra_mmds = torch.diag(mmds, 0)\n",
    "        intra = torch.sum(intra_mmds) / self.num_classes\n",
    "        \n",
    "        inter = None\n",
    "        # trick to create a mask to exclude diagonal like\n",
    "        # False True True\n",
    "        # True False True\n",
    "        # True True False\n",
    "        inter_mask = (torch.ones([len(samples_for_class_source), len(samples_for_class_source)]).to(self.device) - \\\n",
    "            torch.eye(len(samples_for_class_source))).type(torch.bool)\n",
    "        inter_mmds = torch.masked_select(mmds, inter_mask)\n",
    "        # inter is a flatten of all cells except the diagonal\n",
    "        inter = torch.sum(inter_mmds) / (self.num_classes * (self.num_classes - 1))\n",
    "        \n",
    "        cdd = intra - inter\n",
    "        return {\n",
    "            'ccd': cdd,\n",
    "            'intra': intra,\n",
    "            'inter': inter\n",
    "        }\n",
    "\n",
    "        \n",
    "    def activate_layer(\n",
    "        self,\n",
    "        source_features: torch.Tensor, # shape: [batch_size_source X output_of_layer(i)]\n",
    "        target_features: torch.Tensor, # shape: [batch_size_target X output_of_layer(i)]\n",
    "        samples_for_class_source: List[int], # shape: [num_classes]\n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "    ):\n",
    "        assert len(samples_for_class_source) == len(samples_for_class_target), \"number of classes must be the same\"\n",
    "        \n",
    "        num_classes = len(samples_for_class_source)\n",
    "        \n",
    "        # ss means source, source\n",
    "        # tt means target, target\n",
    "        # st means source, target\n",
    "        distances = {}\n",
    "        \n",
    "        distances['ss'] = self.compute_pairwise_distance(source_features, source_features)\n",
    "        distances['tt'] = self.compute_pairwise_distance(target_features, target_features)\n",
    "        distances['st'] = self.compute_pairwise_distance(source_features, target_features)\n",
    "        distances['ss'] = self.extract_classwise_information(\n",
    "            distances['ss'],\n",
    "            samples_for_class_source\n",
    "        )\n",
    "        distances['tt'] = self.extract_classwise_information(\n",
    "            distances['tt'],\n",
    "            samples_for_class_target\n",
    "        )\n",
    "        \n",
    "        mean_distances = self.pairwise_class_mean_distance(\n",
    "            samples_for_class_source,\n",
    "            samples_for_class_target,\n",
    "            distances\n",
    "        )\n",
    "        \n",
    "        return distances, mean_distances\n",
    "        \n",
    "    def compute_pairwise_distance(\n",
    "        self,\n",
    "        ten1: torch.Tensor, # shape [batch_size_1 x features_length]\n",
    "        ten2: torch.Tensor # shape [batch_size_2 x features_length]\n",
    "    ) -> torch.Tensor: # shape [batch_size_1 x batch_size_2 x features_length]\n",
    "        # batch_size 1 and 2 could be different\n",
    "        assert ten1.shape[1] == ten2.shape[1], \"Features length must be the same\"\n",
    "        batch_size_1 = ten1.shape[0]\n",
    "        batch_size_2 = ten2.shape[0]\n",
    "        \n",
    "        features_length = ten1.shape[1]\n",
    "        \n",
    "        # goes from [batch_size_1 x features_length] to [batch_size_1 x batch_size_2 x features_length]\n",
    "        expanded_1 = ten1.unsqueeze(1).expand(batch_size_1, batch_size_2, features_length)\n",
    "        expanded_2 = ten2.unsqueeze(0).expand(batch_size_1, batch_size_2, features_length)\n",
    "        \n",
    "        # sum over the dimension 2 collases features\n",
    "        # goes from [batch_size_1 x batch_size_2 x features_length] to [batch_size_1 x batch_size_2]\n",
    "        squared_distance = ((expanded_1 - expanded_2)**2).sum(2)\n",
    "        return squared_distance\n",
    "    \n",
    "    def extract_classwise_information(\n",
    "        self,\n",
    "        distance: torch.Tensor, # shape: [batch_size x batch_size]\n",
    "        samples_for_class: List[int], # shape: [num_classes]\n",
    "    ) -> List[torch.Tensor]:\n",
    "        # the idea is to create one square submatrix for each class with side\n",
    "        # equal to the amount of samples for that class inside the batch. In this way the\n",
    "        # size of matrices will be proportional to the impact of it inside the batch_size\n",
    "        square_submatrices_length = [length for length in samples_for_class]\n",
    "        \n",
    "        classes_distance = []\n",
    "        x_offset = 0\n",
    "        y_offset = 0\n",
    "        for length in square_submatrices_length:\n",
    "            class_distance = distance[x_offset:x_offset+length, y_offset:y_offset+length]\n",
    "            classes_distance.append(class_distance)\n",
    "            # translation over the diagonal of the matrix\n",
    "            x_offset += length\n",
    "            y_offset += length\n",
    "        return classes_distance\n",
    "    \n",
    "    def pairwise_class_mean_distance(\n",
    "        self,\n",
    "        samples_for_class_source: List[int], # shape: [num_classes] \n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "        distances: Dict[str, torch.Tensor] # shape [batch_size_1 x batch_size_2 x features_length]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # patches will contain the square submatrices computed in extract_classwise_information\n",
    "        patches = {}\n",
    "        mean_distances = {}\n",
    "        mean_distances['st'] = torch.zeros_like(distances['st'], requires_grad=False).to(self.device)\n",
    "        mean_distances['ss'] = []\n",
    "        mean_distances['tt'] = []\n",
    "        for c in range(len(samples_for_class_source)):\n",
    "            mean_distances['ss'] += [\n",
    "                torch.zeros(\n",
    "                    len(samples_for_class_source), requires_grad=False\n",
    "                ).to(self.device)\n",
    "            ]\n",
    "            mean_distances['tt'] += [\n",
    "                torch.zeros(\n",
    "                    len(samples_for_class_source), requires_grad=False\n",
    "                ).to(self.device)\n",
    "            ]\n",
    "        \n",
    "        classes_distance = []\n",
    "        source_start = 0\n",
    "        source_end = 0\n",
    "        for source_class_index in range(len(samples_for_class_source)):\n",
    "            source_stard = source_end\n",
    "            source_end = source_stard + samples_for_class_source[source_class_index]\n",
    "            patches['ss'] = distances['ss'][source_class_index]\n",
    "            \n",
    "            target_start = 0\n",
    "            target_end = 0\n",
    "            for target_class_index in range(len(samples_for_class_target)):\n",
    "                target_start = target_end\n",
    "                target_end = target_start + samples_for_class_target[target_class_index]\n",
    "                patches['tt'] = distances['tt'][target_class_index]\n",
    "                \n",
    "                patches['st'] = distances['st'][\n",
    "                    source_start:source_start + samples_for_class_source[source_class_index],\n",
    "                    target_start:target_start + samples_for_class_target[target_class_index]\n",
    "                ]\n",
    "                \n",
    "                mean = self.mean_estimation(patches)\n",
    "                \n",
    "                mean_distances['ss'][source_class_index][target_class_index] = mean\n",
    "                mean_distances['tt'][source_class_index][target_class_index] = mean\n",
    "                mean_distances['st'][source_stard:source_end, target_start:target_end] = mean\n",
    "\n",
    "        return mean_distances\n",
    "    \n",
    "    def mean_estimation(\n",
    "        self,\n",
    "        distances_patches: Dict[str, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        patches_sum = torch.sum(distances_patches['ss']) + \\\n",
    "            torch.sum(distances_patches['tt']) + \\\n",
    "            torch.sum(distances_patches['st'])\n",
    "        \n",
    "        batch_size_source = distances_patches['ss'].shape[0]\n",
    "        batch_size_target = distances_patches['tt'].shape[0]\n",
    "        \n",
    "        # batch_size_source and batch_size_taret are removed in order\n",
    "        # to obtain an unbiased estimator since total_amount_values\n",
    "        # will be used as denominator for the mean computation\n",
    "        total_amount_values = (batch_size_source * batch_size_source) + \\\n",
    "            (batch_size_target * batch_size_target) + \\\n",
    "            2 * (batch_size_source * batch_size_target) - \\\n",
    "            batch_size_source - batch_size_target\n",
    "        \n",
    "        mean = patches_sum.item() / total_amount_values\n",
    "        return mean\n",
    "    \n",
    "    def compute_cum_kernel_dist(\n",
    "        self,\n",
    "        layer_dist_st: List[torch.Tensor], # shape: [batch_size_source X batch_size_target]\n",
    "        layer_mean_st: List[torch.Tensor], # shape: [batch_size_source X batch_size_target]\n",
    "        class_index: Optional[int]=None\n",
    "    ):\n",
    "        cum_kernel_dist = None\n",
    "        for i, order, multiplier in zip(\n",
    "            range(self.num_layers),\n",
    "            self.kernel_orders,\n",
    "            self.kernel_multipliers\n",
    "        ):\n",
    "            distance = layer_dist_st[i] if class_index is None else layer_dist_st[i][class_index]\n",
    "            mean = layer_mean_st[i] if class_index is None else layer_mean_st[i][class_index]\n",
    "            \n",
    "            kernel_dist = self.compute_kernel_dist(distance, mean, order, multiplier)\n",
    "            if cum_kernel_dist is None:\n",
    "                cum_kernel_dist = kernel_dist\n",
    "            else:\n",
    "                cum_kernel_dist += kernel_dist\n",
    "                \n",
    "        return cum_kernel_dist\n",
    "      \n",
    "    def compute_kernel_dist(\n",
    "        self,\n",
    "        distance: torch.Tensor, # shape: [batch_size_source X batch_size_target]\n",
    "        mean: torch.Tensor, # shape: [batch_size_source X batch_size_target]\n",
    "        kernel_order: int,\n",
    "        kernel_multiplier: int\n",
    "    ):\n",
    "        base_mean = mean / (kernel_multiplier ** (kernel_order // 2))\n",
    "        mean_list = [base_mean * (kernel_multiplier**i) for i in range(kernel_order)]\n",
    "        mean_tensor = torch.stack(mean_list, dim=0).to(self.device)\n",
    "\n",
    "        eps = 1e-5\n",
    "        mean_mask = (mean_tensor < eps).type(torch.FloatTensor)\n",
    "        mean_tensor = (1.0 - mean_mask) * mean_tensor + mean_mask * eps \n",
    "        mean_tensor = mean_tensor.detach()\n",
    "\n",
    "        for i in range(len(mean_tensor.size()) - len(distance.size())):\n",
    "            distance = distance.unsqueeze(0)\n",
    "\n",
    "        distance = distance / mean_tensor\n",
    "        upper_mask = (distance > 1e5).type(torch.FloatTensor).detach()\n",
    "        lower_mask = (distance < 1e-5).type(torch.FloatTensor).detach()\n",
    "        normal_mask = 1.0 - upper_mask - lower_mask\n",
    "        distance = normal_mask * distance + upper_mask * 1e5 + lower_mask * 1e-5\n",
    "        kernel_val = torch.sum(torch.exp(-1.0 * distance), dim=0)\n",
    "        return kernel_val\n",
    "    \n",
    "    def classes_mean_kernel(\n",
    "        self,\n",
    "        samples_for_class_source: List[int], # shape: [num_classes] \n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "        cumulate_kernel: torch.Tensor # shape: [batch_size_source X batch_size_target]\n",
    "    ) -> torch.Tensor: # shape: [num_classes X num_classes]\n",
    "        mean = torch.zeros(\n",
    "            (len(samples_for_class_source), len(samples_for_class_source))\n",
    "        ).to(self.device)\n",
    "        x_start = 0\n",
    "        x_end = 0\n",
    "        for source_class_index in range(len(samples_for_class_source)):\n",
    "            x_start = x_end\n",
    "            x_end = x_start + samples_for_class_source[source_class_index]\n",
    "            \n",
    "            y_start = 0\n",
    "            y_end = 0\n",
    "            for target_class_index in range(len(samples_for_class_target)):\n",
    "                y_start = y_end\n",
    "                y_end = y_start + samples_for_class_target[target_class_index]\n",
    "                \n",
    "                class_mean = torch.mean(\n",
    "                    cumulate_kernel[\n",
    "                        x_start:x_start + samples_for_class_source[source_class_index],\n",
    "                        y_start:y_start + samples_for_class_target[target_class_index]\n",
    "                    ]\n",
    "                )\n",
    "                mean[source_class_index, target_class_index] = class_mean\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055f7b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2e95c",
   "metadata": {},
   "source": [
    "All models are composed by three main elements:\n",
    "\n",
    "- backbone\n",
    "- feature processor\n",
    "- classifier\n",
    "\n",
    "The backbone is a pretrained network from `torchvision` trained with a smaller learning rate than the one used for other two components.\n",
    "The feature extractor is used for process the output of the backbone, we decided to separate these two steps into two classes in order to have more control on both of them, this allow to make tests with many combinations.\n",
    "The last element is the classifier, it is a simple linear layer that takes the output of backbone (feature processor if it is used) and return an array of prediction with the same length of the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb394a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ac533",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_type: str,\n",
    "    ) -> None:\n",
    "        super(Backbone, self).__init__()\n",
    "        if backbone_type == \"resnet34\":\n",
    "            self.backbone = models.resnet34(pretrained=True)\n",
    "            self.output_num = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Sequential()\n",
    "        else:\n",
    "            raise ValueError(\"select one valid backbone_type\")\n",
    "            \n",
    "        \n",
    "    def get_output_num(self):\n",
    "        return self.output_num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "class FeatureProcessor(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        super(FeatureProcessor, self).__init__()\n",
    "        self.processing_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_num = output_size\n",
    "        \n",
    "    def get_output_num(self):\n",
    "        return self.output_num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.processing_layer(x)\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_type: str,\n",
    "        n_classes: int,\n",
    "        n_feature_extracted: Optional[int] = None\n",
    "    ) -> None:\n",
    "        super(Classifier, self).__init__()\n",
    "        self.backbone = Backbone(backbone_type)\n",
    "        if n_feature_extracted:\n",
    "            self.processing_features = True\n",
    "            self.processing_layer = FeatureProcessor(self.backbone.get_output_num(), n_feature_extracted)\n",
    "        else:\n",
    "            self.processing_features = False\n",
    "        self.classification_layer = nn.Linear(\n",
    "            self.processing_layer.get_output_num() if self.processing_features else self.backbone.get_output_num(),\n",
    "            n_classes\n",
    "        )\n",
    "        self.output_num = n_classes\n",
    "        \n",
    "    def get_output_num(self):\n",
    "        self.output_num\n",
    "        \n",
    "    def get_features(self, x):\n",
    "        features = self.backbone(x)\n",
    "        if self.processing_features:\n",
    "            features = self.processing_layer(features)\n",
    "        return features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.get_features(x)\n",
    "        classes = self.classification_layer(features)\n",
    "        \n",
    "        return features, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e68f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181e059",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc7459",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "source_dataset = ImageFolder(\n",
    "    DATASET_PATH / \"product_images\",\n",
    "    transform=T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "target_dataset = ImageFolder(\n",
    "    DATASET_PATH / \"real_life\",\n",
    "    transform=T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation_ratio = .2\n",
    "test_ratio = .2\n",
    "\n",
    "validation_dataset, test_dataset, train_dataset = random_split(\n",
    "    source_dataset,\n",
    "    [\n",
    "        int(len(source_dataset)*validation_ratio),\n",
    "        int(len(source_dataset)*test_ratio),\n",
    "        len(source_dataset) - (\n",
    "            int(len(source_dataset)*validation_ratio)\n",
    "            +int(len(source_dataset)*test_ratio)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_test_ratio = .4\n",
    "target_test_dataset, target_train_dataset = random_split(\n",
    "    target_dataset,\n",
    "    [\n",
    "        int(len(target_dataset)*target_test_ratio),\n",
    "        len(target_dataset) - int(len(target_dataset)*target_test_ratio)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c8ee9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset), len(target_train_dataset), len(target_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606c41f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf5a20",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler, SGD, Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb2b01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# misc\n",
    "set_random_seed(33)\n",
    "device = get_device()\n",
    "num_threads = 4\n",
    "\n",
    "# train\n",
    "num_epochs = 30\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "weight_decay = 0.000001\n",
    "momentum = 0.9\n",
    "scheduler_factor = 0.25\n",
    "scheduler_patience = 20\n",
    "\n",
    "# domain adaptation loss\n",
    "use_coral = True\n",
    "coral_weight = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd88f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, num_threads):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_threads\n",
    "    )\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum):\n",
    "    higher_lr_weights = []\n",
    "    lower_lr_weights = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('backbone'):\n",
    "            lower_lr_weights.append(param)\n",
    "        elif name.startswith('processing') or name.startswith('classification'):\n",
    "            higher_lr_weights.append(param)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected type of weights {name}\")\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [\n",
    "            {'params': lower_lr_weights},\n",
    "            {'params': higher_lr_weights, 'lr': lr}\n",
    "        ],\n",
    "        lr=lr / 10,\n",
    "        weight_decay=wd,\n",
    "        momentum=momentum\n",
    "    )\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063aa8d7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "        model = Classifier(\n",
    "    backbone_type=\"resnet34\", \n",
    "    n_classes=len(dataset.classes),\n",
    "    # n_feature_extracted=2048\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = get_optimizer(model, lr, weight_decay, momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    'min',\n",
    "    factor=scheduler_factor,\n",
    "    patience=scheduler_patience,\n",
    "    verbose=True\n",
    ") \n",
    "dataloaders = {\n",
    "    'train': {\n",
    "        'source': get_data_loader(train_dataset, batch_size, num_threads),\n",
    "        'target': get_data_loader(target_train_dataset, batch_size, num_threads),\n",
    "    },\n",
    "    'validation': {\n",
    "        'source': get_data_loader(validation_dataset, batch_size, num_threads),\n",
    "    }\n",
    "    #'test': get_data_loader(test_dataset, batch_size, num_threads)\n",
    "}\n",
    "#summary(model, input_size=(batch_size, *dataset[0][0].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = model\n",
    "best_loss = np.Inf\n",
    "\n",
    "phases_loss = {k: [] for k in dataloaders.keys()}\n",
    "phases_acc = {k: [] for k in dataloaders.keys()}\n",
    "for epoch in range(num_epochs):\n",
    "    for phase, phase_dataloaders in dataloaders.items():\n",
    "        source_dataloader = phase_dataloaders['source']\n",
    "        target_dataloader = phase_dataloaders.get('target', None)\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        for index, (x, labels) in enumerate(source_dataloader):\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=device.type):\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    features, predictions = model(x)\n",
    "                    x = x.detach().cpu()\n",
    "                    \n",
    "                    loss = criterion(predictions, labels)\n",
    "                    predictions = predictions.detach().cpu()\n",
    "                    labels = labels.detach().cpu()\n",
    "                    if use_coral and not target_dataloader is None:\n",
    "                        source_feauteres = features.cpu()\n",
    "                        target_x, _ = next(target_dataloader._get_iterator())\n",
    "                        target_x = target_x.to(device)\n",
    "                        target_fuatures, _ = model(target_x)\n",
    "                        target_x = target_x.detach().cpu()\n",
    "                        target_fuatures = target_fuatures.cpu()\n",
    "                        loss = loss + coral_weight * coral(source_feauteres, target_fuatures)\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += (torch.argmax(predictions, 1) == labels).sum().item() / labels.size()[0]\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "        epoch_loss /= len(source_dataloader)\n",
    "        epoch_acc /= len(source_dataloader)\n",
    "        phases_loss[phase].append(epoch_loss)\n",
    "        phases_acc[phase].append(epoch_acc)\n",
    "\n",
    "        if phase == \"validation\" and abs(epoch_loss) <= abs(best_loss):\n",
    "            best_model = model\n",
    "            best_loss = epoch_loss\n",
    "            scheduler.step(epoch_loss)\n",
    "            \n",
    "    print_epoch_chart(\n",
    "        phases_loss['train'],\n",
    "        phases_loss['validation'],\n",
    "        phases_acc['train'],\n",
    "        phases_acc['validation'],\n",
    "        epoch,\n",
    "        num_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1261a",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7ff39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "reals = []\n",
    "for x, y in test_dataset:\n",
    "    x_gpu = x.unsqueeze(0).to(device)\n",
    "    _, predicted = best_model(x_gpu)\n",
    "    predictions.append(np.argmax(predicted.detach().cpu().numpy()))\n",
    "    reals.append(y)\n",
    "    del x_gpu\n",
    "predictions = np.array(predictions)\n",
    "reals = np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3744a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(index, idx_to_class, dataset, reals, predictions):\n",
    "    x, _ = dataset[index]\n",
    "    plt.imshow(x[1])\n",
    "    print(idx_to_class[reals[index]], idx_to_class[predictions[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11000b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictions(350, idx_to_class, test_dataset, reals, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c74b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(reals == predictions).sum() / reals.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30b96d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a15c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6caa3c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "reals = []\n",
    "for x, y in target_dataset:\n",
    "    x_gpu = x.unsqueeze(0).to(device)\n",
    "    _, predicted = best_model(x_gpu)\n",
    "    predictions.append(np.argmax(predicted.detach().cpu().numpy()))\n",
    "    reals.append(y)\n",
    "    del x_gpu\n",
    "predictions = np.array(predictions)\n",
    "reals = np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5032554",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(index, idx_to_class, dataset, reals, predictions):\n",
    "    x, _ = dataset[index]\n",
    "    plt.imshow(x[1])\n",
    "    print(idx_to_class[reals[index]], idx_to_class[predictions[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f47c70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictions(350, idx_to_class, target_dataset, reals, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e706e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(reals == predictions).sum() / reals.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "name": "project.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
