{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bc6b70",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185850f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATASET_ZIP_PATH = '/tmp/Adaptiope.zip'\n",
    "DATASET_EXTRACTION_PATH = '/tmp/Adaptiope'\n",
    "DATASET_PATH = Path('./data/adaptiope_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a99d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b451f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join, isdir\n",
    "from shutil import copytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113c3037",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p {DATASET_EXTRACTION_PATH}\n",
    "!unzip -d {DATASET_EXTRACTION_PATH} {DATASET_ZIP_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b8dda",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "if not isdir(DATASET_PATH):\n",
    "    classes = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
    "               \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
    "               \"purse\", \"stand mixer\", \"stroller\"]\n",
    "    for d, td in zip([\n",
    "        f\"{DATASET_EXTRACTION_PATH}/Adaptiope/product_images\",\n",
    "        f\"{DATASET_EXTRACTION_PATH}/Adaptiope/real_life\"],[\n",
    "        f\"{DATASET_PATH}/product_images\",\n",
    "        f\"{DATASET_PATH}/real_life\"]):\n",
    "        makedirs(td)\n",
    "        for c in classes:\n",
    "            c_path = join(d, c)\n",
    "            c_target = join(td, c)\n",
    "            copytree(c_path, c_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44aef08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f93276",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd1b5d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = ImageFolder(DATASET_PATH / \"product_images\")\n",
    "idx_to_class = {v: k for k,v in dataset.class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f452664d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imgs: List[str, int] path, class\n",
    "seen_classes = set()\n",
    "imgs = []\n",
    "for i, (p, c) in enumerate(dataset.imgs):\n",
    "    if c not in seen_classes:\n",
    "        seen_classes.add(c)\n",
    "        imgs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39292dd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 5, figsize=(10,10))\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        image, title = dataset[imgs.pop(0)]\n",
    "        axs[i,j].imshow(image)\n",
    "        axs[i,j].set_title(idx_to_class[title])\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d402fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c1bed",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def set_random_seed(seed=0) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def print_epoch_chart(\n",
    "    train_losses,\n",
    "    val_losses,\n",
    "    train_acc,\n",
    "    val_acc,\n",
    "    epoch,\n",
    "    num_epochs\n",
    "):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    fig, (loss_ax, acc_ax) = plt.subplots(1, 2, figsize=(15,7))\n",
    "    x = np.arange(len(train_losses))\n",
    "    loss_ax.plot(x, train_losses, label='train')\n",
    "    loss_ax.plot(x, val_losses, label='validation')\n",
    "    loss_ax.set_title(\"Loss\")\n",
    "    loss_ax.set_ylim([0, 5])\n",
    "    loss_ax.legend()\n",
    "    acc_ax.plot(x, train_acc, label='train')\n",
    "    acc_ax.plot(x, val_acc, label='validation')\n",
    "    acc_ax.set_title(\"Accuracy\")\n",
    "    acc_ax.legend()\n",
    "    fig.suptitle(f\"Epoch {epoch+1} of {num_epochs}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a19051",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46672b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def coral(source_extracted_features, target_extracted_features, **kwargs):\n",
    "    source = source_extracted_features.double()\n",
    "    target = target_extracted_features.double()\n",
    "    \n",
    "    d = source.data.shape[1]\n",
    "    ns, nt = source.data.shape[0], target.data.shape[0]\n",
    "    # source covariance\n",
    "    xm = torch.mean(source, 0, keepdim=True) - source\n",
    "    xc = xm.t() @ xm / (ns - 1)\n",
    "\n",
    "    # target covariance\n",
    "    xmt = torch.mean(target, 0, keepdim=True) - target\n",
    "    xct = xmt.t() @ xmt / (nt - 1)\n",
    "\n",
    "    # frobenius norm between source and target\n",
    "    loss = torch.mul((xc - xct), (xc - xct))\n",
    "    loss = torch.sum(loss) / (4*d*d)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c4dc7-22c9-497f-b216-dcf35c2f2d26",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contrastive Domain Discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7519abf4-3682-4823-8af7-7119fe5265a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\\require{color}$\n",
    "*Contrastive Domain Discrepancy (CDD)* is a measure used to quantify the dissimilarity between classes with respect to source and target domains. The computation is made on features extracted from linear layers of the network after the pretrained backbone module.\n",
    "\n",
    "\\begin{equation}\n",
    "e_1 = \\sum_{i=1}^{n_s}\\sum_{j=1}^{n_s} \\frac{\\mu_{c_1c_1}(y_i^s, y_j^s)k(\\phi(x_i^s), \\phi(x_j^s))} { \\sum_{i=1}^{n_s}\\sum_{j=1}^{n_s}\\mu_{c_1c_1}(y_i^s, y_j^s)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "e_2 = \\sum_{i=1}^{n_t}\\sum_{j=1}^{n_t} \\frac{\\mu_{c_2c_2}(y_i^t, y_j^t)k(\\phi(x_i^t), \\phi(x_j^t))} {\\sum_{i=1}^{n_t}\\sum_{j=1}^{n_t}\\mu_{c_2c_2}(y_i^t, y_j^t)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "e_3 = \\sum_{i=1}^{n_s}\\sum_{j=1}^{n_t} \\frac{\\mu_{c_1c_2}(y_i^s, y_j^t)k(\\phi(x_i^s), \\phi(x_j^t))} {\\sum_{i=1}^{n_s}\\sum_{j=1}^{n_t}\\mu_{c_1c_2}(y_i^s, y_j^t)}\n",
    "\\end{equation}\n",
    "\n",
    "Suppose the two following tables contain the outputs of the last two linear layers of the network for the source dataset with a batch size equal to 5; the layer sizes are respectively 5 for the first and to 3 for the second: rows represent samples, and columns are the features.\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "    \\hline\n",
    "    0.496257 & 0.768222 & 0.0884774 & 0.13203  & 0.307423 \\\\\n",
    "    \\hline\n",
    "    0.634079 & 0.490093 & 0.896445  & 0.455628 & 0.632306 \\\\\n",
    "    \\hline\n",
    "    0.348893 & 0.401717 & 0.0223258 & 0.168859 & 0.293888 \\\\\n",
    "    \\hline\n",
    "    0.518522 & 0.697668 & 0.800011  & 0.161029 & 0.282269 \\\\\n",
    "    \\hline\n",
    "    0.681609 & 0.915194 & 0.3971    & 0.874156 & 0.419408 \\\\\n",
    "    \\hline\n",
    "\\end{array}\\quad\\quad\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "    \\hline\n",
    "    0.552907 & 0.952738  & 0.0361648 \\\\\n",
    "    \\hline\n",
    "    0.185231 & 0.373417  & 0.3051    \\\\\n",
    "    \\hline\n",
    "    0.932    & 0.17591   & 0.269834  \\\\\n",
    "    \\hline\n",
    "    0.15068  & 0.0317195 & 0.20813   \\\\\n",
    "    \\hline\n",
    "    0.929799 & 0.723109  & 0.742336  \\\\\n",
    "    \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Same for the target dataset with a batch size equal to 4.\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "    \\hline\n",
    "    0.526296 & 0.243658 & 0.584592 & 0.0331526 & 0.138717 \\\\\n",
    "    \\hline\n",
    "    0.242235 & 0.815469 & 0.793161 & 0.278252  & 0.481959 \\\\\n",
    "    \\hline\n",
    "    0.81978  & 0.997067 & 0.698441 & 0.567546  & 0.835243 \\\\\n",
    "    \\hline\n",
    "    0.205599 & 0.593172 & 0.112347 & 0.153457  & 0.241708 \\\\\n",
    "    \\hline\n",
    "\\end{array}\\quad\\quad\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    "0.726237 & 0.70108  & 0.203824 \\\\\n",
    "\\hline\n",
    "0.651054 & 0.774486 & 0.436891 \\\\\n",
    "\\hline\n",
    "0.519091 & 0.615852 & 0.810188 \\\\\n",
    "\\hline\n",
    "0.980097 & 0.114688 & 0.316765 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In these matrices, samples are ordered according to their respective class.\n",
    "\n",
    "For each one of the two domains, it is also required a list describing the number of samples for each class: the *i*-th value of the list represents the number of samples for the class *i*.\n",
    "In this case, we have $[2,2,1]$ for the source dataset and $[1, 1, 2]$ for the target dataset: note that both lists sum up respectively to 5 and 4, which are the sizes of the two minibatches.\n",
    "\n",
    "#### Distance computation\n",
    "The first step is to compute distances among:\n",
    "\n",
    "- source/source dataset\n",
    "- target/target dataset\n",
    "- source/target dataset\n",
    "\n",
    "All these three distances are computed for the output of each of the two linear layers, aggregating for the different number of features summing them (`compute_pairwise_distance` function).\n",
    "\n",
    "Obtained result for distance source/target with shape equal to `[batch size source X batch size target]`:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "Layer 1& Layer2 \\\\\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " 0.560438 & 0.615181 & 0.997362 & 0.120472  \\\\\n",
    " \\hline\n",
    " 0.591715 & 0.324145 & 0.384421 & 1.0529    \\\\\n",
    " \\hline\n",
    " 0.415092 & 0.82409  & 1.48532  & 0.0682521 \\\\\n",
    " \\hline\n",
    " 0.28955  & 0.143876 & 0.66175  & 0.583425  \\\\\n",
    " \\hline\n",
    " 1.29631  & 0.718872 & 0.383529 & 0.962352  \\\\\n",
    " \\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " 0.121484 & 0.201988 & 0.713748 & 0.963555   \\\\\n",
    " \\hline\n",
    " 0.410307 & 0.395216 & 0.425351 & 0.698889   \\\\\n",
    " \\hline\n",
    " 0.3225   & 0.465132 & 0.656027 & 0.00826399 \\\\\n",
    " \\hline\n",
    " 0.779328 & 0.854408 & 0.839412 & 0.706618   \\\\\n",
    " \\hline\n",
    " 0.331919 & 0.173635 & 0.184789 & 0.553817   \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The output for source/source and target/target has respectively shape of `[batch size source X batch size source]` and `[batch size target X batch size target]`. Distances within the same dataset (source/source, target/target) require an extra step that iterates in the distance matrix over the diagonal, extracting square submatrices that contain all distances from samples belonging the same class. Obtained result for one layer of source/source distance (given distribution $[2,2,1]$):\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "Class1&Class2&Class3  \\\\\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    " 0        & 0.543124 \\\\\n",
    " \\hline\n",
    " 0.543124 & 0        \\\\\n",
    "\\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    " 0       & 0.63506 \\\\\n",
    " \\hline\n",
    " 0.63506 & 0       \\\\\n",
    "\\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    " 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The distance matrix source/target is averaged over classes submatrices (colors are used to highlight class groups) creating a new mean matrix:\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\text{Layer1 before the average}&\\text{Layer1 after}\\\\\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " \\color{red}{0.560438} & \\color{green}{0.615181} & \\color{orange}{0.99736} & \\color{orange}{0.120472}  \\\\\n",
    " \\hline\n",
    " \\color{red}{0.591715} & \\color{green}{0.324145} & \\color{orange}{0.384421} & \\color{orange}{1.0529}    \\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.415092 }& \\color{brown}{0.82409  }& \\color{olive}{1.48532  }& \\color{olive}{0.0682521} \\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.28955  }& \\color{brown}{0.143876 }& \\color{olive}{0.66175  }& \\color{olive}{0.583425 } \\\\\n",
    " \\hline\n",
    " \\color{pink}{1.29631  }& \\color{darkgrey}{0.718872 }& \\color{teal}{0.383529 }& \\color{teal}{0.962352 } \\\\\n",
    " \\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    " \\color{red}{0.511834} & \\color{green}{0.476363} & \\color{orange}{0.607436} & \\color{orange}{0.607436} \\\\\n",
    " \\hline\n",
    " \\color{red}{0.511834} & \\color{green}{0.476363} & \\color{orange}{0.607436} & \\color{orange}{0.607436} \\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.432476 }& \\color{brown}{0.397005 }& \\color{olive}{0.567757 }& \\color{olive}{0.567757 }\\\\\n",
    " \\hline\n",
    " \\color{magenta}{0.432476 }& \\color{brown}{0.397005 }& \\color{olive}{0.567757 }& \\color{olive}{0.567757 }\\\\\n",
    " \\hline\n",
    " \\color{pink}{0.280219 }& \\color{darkgrey}{0.307591 }& \\color{teal}{0.655509 }& \\color{teal}{0.655509 }\\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The computation of the mean subregions is also applied to source/source and target/target data.\n",
    "\n",
    "#### Kernel computation\n",
    "Distances and mean matrices are then used to compute a kernel for each layer, and the list of results are summed up. The final matrix has a shape of `[number of samples X number of samples]` and expresses the correlation between samples taken into consideration (in the following table source/target table is presented):\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    " 3.92554 & 3.90333 & 3.65386 \\\\\n",
    " \\hline\n",
    " 3.52906 & 3.25892 & 4.40912 \\\\\n",
    " \\hline\n",
    " 0.74786 & 2.26064 & 4.51405 \\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### Maximum Mean Discrepancy (MMD) computation\n",
    "The three kernels extracted from source/source, targe/target and source/target are composed together to obtaind the MMD:\n",
    "\n",
    "MMD formulas\n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{D}}^{mmd}_{l} &= \\frac{1}{n_s^2}\\sum_{i=1}^{n_s}\\sum_{j=1}^{n_s}k_l(\\phi_l(x_i^s), \\phi_l(x_j^s)) \\\\\n",
    "& + \\frac{1}{n_t^2}\\sum_{i=1}^{n_t}\\sum_{j=1}^{n_t}k_l(\\phi_l(x_i^t), \\phi_l(x_j^t)) \\\\\n",
    "& - \\frac{1}{n_s n_t}\\sum_{i=1}^{n_t}\\sum_{j=1}^{n_t}k_l(\\phi_l(x_i^s), \\phi_l(x_j^t))\n",
    "\\end{align}\n",
    "\n",
    "The result is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "  8.30697 &  8.33474 & 4.42708 \\\\\n",
    "  \\hline\n",
    "  9.13727 &  9.64986 & 3.07151 \\\\\n",
    "  \\hline\n",
    " 18.5041  & 15.4785  & 7.26769 \\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where the diagonal represents the intra class discrepancy and the rest of the matrix the inter class discrepancy.\n",
    "\n",
    "$$\n",
    "\\begin{array}{cc}\n",
    "\\text{Intra class elements}&\\text{inter class elements}\\\\\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "  8.30697 &  \\color{lightgrey}{8.33474} & \\color{lightgrey}{4.42708} \\\\\n",
    "  \\hline\n",
    "  \\color{lightgrey}{9.13727} &  9.64986 & \\color{lightgrey}{3.07151} \\\\\n",
    "  \\hline\n",
    "  \\color{lightgrey}{18.5041}  & \\color{lightgrey}{15.4785}  & 7.26769 \\\\\n",
    " \\hline\n",
    "\\end{array}&\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "  \\color{lightgrey}{8.30697} &  8.33474 & 4.42708 \\\\\n",
    "  \\hline\n",
    "  9.13727 &  \\color{lightgrey}{9.64986} & 3.07151 \\\\\n",
    "  \\hline\n",
    " 18.5041  & 15.4785  & \\color{lightgrey}{7.26769} \\\\\n",
    " \\hline\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### CDD, intra class, infra class computation\n",
    "The intra and inter class measures can be computed averaging over vectors of the previous step. The CDD instead can be obtained applying the following formula:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathcal{D}}^{cdd}_{l} &= \\underbrace{\\frac{1}{M}\\sum_{c=1}^M\\hat{\\mathcal{D}}^{cc}(\\hat{y}_{1:n_t}^t, \\phi)}_\\text{intra class} \\\\\n",
    "&- \\underbrace{\\frac{1}{M(M -1)} \\sum_{c=1}^M\\sum_{c'=1\\\\c'\\ne c}^M\\hat{\\mathcal{D}}^{cc'}(\\hat{y}_{1:n_t}^t, \\phi)}_\\text{inter class}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b465f3b0-4d96-40c0-8e0a-0ceb7ceb5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict, Optional\n",
    "\n",
    "class ConstrativeDomainDiscrepancy():\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        kernel_orders: Tuple[int],\n",
    "        kernel_multipliers: Tuple[int],\n",
    "        num_classes: int,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_orders = kernel_orders\n",
    "        self.kernel_multipliers = kernel_multipliers\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        assert num_layers == len(kernel_orders) == len(kernel_multipliers), \"There must be one kernel order and multiplier for each layer\"\n",
    "    \n",
    "    def activate(\n",
    "        self,\n",
    "        source_layers_out: List[torch.Tensor], # shape: [num_layers X batch_size_source X output_of_layer(i)]\n",
    "        target_layers_out: List[torch.Tensor], # shape: [num_layers X batch_size_target X output_of_layer(i)]\n",
    "        samples_for_class_source: List[int], # shape: [num_classes]\n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "    ) -> Dict[str, float]:\n",
    "        assert len(source_layers_out) == len(target_layers_out) == self.num_layers, \"number of layers must be the same\"\n",
    "\n",
    "        layers_dist = []\n",
    "        layers_mean = []\n",
    "        for src, trg in zip(source_layers_out, target_layers_out):\n",
    "            layer_distances, layer_mean_distances = self.activate_layer(\n",
    "                src,\n",
    "                trg,\n",
    "                samples_for_class_source,\n",
    "                samples_for_class_target\n",
    "            )\n",
    "            layers_dist.append(layer_distances)\n",
    "            layers_mean.append(layer_mean_distances)\n",
    "            \n",
    "        for i in range(self.num_layers):\n",
    "            for c in range(len(samples_for_class_source)):\n",
    "                # shape goes from [num_classes] to [num_classes X 1 X 1]\n",
    "                layers_mean[i]['ss'][c] = layers_mean[i]['ss'][c].view(\n",
    "                    len(samples_for_class_source), 1, 1\n",
    "                )\n",
    "                layers_mean[i]['tt'][c] = layers_mean[i]['tt'][c].view(\n",
    "                    len(samples_for_class_source), 1, 1\n",
    "                )\n",
    "        \n",
    "        class_kernel_dist = self.classes_mean_kernel(\n",
    "            samples_for_class_source,\n",
    "            samples_for_class_target,\n",
    "            self.compute_cum_kernel_dist(\n",
    "                [i['st'] for i in layers_dist],\n",
    "                [i['st'] for i in layers_mean]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        cum_kernel_dist_ss = []\n",
    "        cum_kernel_dist_tt = []\n",
    "        for class_index in range(len(samples_for_class_source)):\n",
    "            cum_kernel_dist_ss.append(\n",
    "                torch.mean(\n",
    "                    self.compute_cum_kernel_dist(\n",
    "                        [i['ss'] for i in layers_dist],\n",
    "                        [i['ss'] for i in layers_mean],\n",
    "                        class_index\n",
    "                    ).view(len(samples_for_class_source), -1), dim=1\n",
    "                )\n",
    "            )\n",
    "            cum_kernel_dist_tt.append(\n",
    "                torch.mean(\n",
    "                    self.compute_cum_kernel_dist(\n",
    "                        [i['tt'] for i in layers_dist],\n",
    "                        [i['tt'] for i in layers_mean],\n",
    "                        class_index\n",
    "                    ).view(len(samples_for_class_source), -1), dim=1\n",
    "                )\n",
    "            )\n",
    "        kernel_dist_ss = torch.stack(cum_kernel_dist_ss, dim=0)\n",
    "        # understand why transpose here\n",
    "        kernel_dist_tt = torch.stack(cum_kernel_dist_tt, dim=0).transpose(1, 0)\n",
    "        \n",
    "        mmds = kernel_dist_ss + kernel_dist_tt - 2 * class_kernel_dist\n",
    "        \n",
    "        intra_mmds = torch.diag(mmds, 0)\n",
    "        intra = torch.sum(intra_mmds) / self.num_classes\n",
    "        \n",
    "        inter = None\n",
    "        # trick to create a mask to exclude diagonal like\n",
    "        # False True True\n",
    "        # True False True\n",
    "        # True True False\n",
    "        inter_mask = (torch.ones([len(samples_for_class_source), len(samples_for_class_source)]).to(self.device) - \\\n",
    "            torch.eye(len(samples_for_class_source))).type(torch.bool)\n",
    "        inter_mmds = torch.masked_select(mmds, inter_mask)\n",
    "        # inter is a flatten of all cells except the diagonal\n",
    "        inter = torch.sum(inter_mmds) / (self.num_classes * (self.num_classes - 1))\n",
    "        \n",
    "        cdd = intra - inter\n",
    "        return {\n",
    "            'ccd': cdd,\n",
    "            'intra': intra,\n",
    "            'inter': inter\n",
    "        }\n",
    "\n",
    "        \n",
    "    def activate_layer(\n",
    "        self,\n",
    "        source_features: torch.Tensor, # shape: [batch_size_source X output_of_layer(i)]\n",
    "        target_features: torch.Tensor, # shape: [batch_size_target X output_of_layer(i)]\n",
    "        samples_for_class_source: List[int], # shape: [num_classes]\n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "    ):\n",
    "        assert len(samples_for_class_source) == len(samples_for_class_target), \"number of classes must be the same\"\n",
    "        \n",
    "        num_classes = len(samples_for_class_source)\n",
    "        \n",
    "        # ss means source, source\n",
    "        # tt means target, target\n",
    "        # st means source, target\n",
    "        distances = {}\n",
    "        \n",
    "        distances['ss'] = self.compute_pairwise_distance(source_features, source_features)\n",
    "        distances['tt'] = self.compute_pairwise_distance(target_features, target_features)\n",
    "        distances['st'] = self.compute_pairwise_distance(source_features, target_features)\n",
    "        distances['ss'] = self.extract_classwise_information(\n",
    "            distances['ss'],\n",
    "            samples_for_class_source\n",
    "        )\n",
    "        distances['tt'] = self.extract_classwise_information(\n",
    "            distances['tt'],\n",
    "            samples_for_class_target\n",
    "        )\n",
    "        \n",
    "        mean_distances = self.pairwise_class_mean_distance(\n",
    "            samples_for_class_source,\n",
    "            samples_for_class_target,\n",
    "            distances\n",
    "        )\n",
    "        \n",
    "        return distances, mean_distances\n",
    "        \n",
    "    def compute_pairwise_distance(\n",
    "        self,\n",
    "        ten1: torch.Tensor, # shape [batch_size_1 x features_length]\n",
    "        ten2: torch.Tensor # shape [batch_size_2 x features_length]\n",
    "    ) -> torch.Tensor: # shape [batch_size_1 x batch_size_2 x features_length]\n",
    "        # batch_size 1 and 2 could be different\n",
    "        assert ten1.shape[1] == ten2.shape[1], \"Features length must be the same\"\n",
    "        batch_size_1 = ten1.shape[0]\n",
    "        batch_size_2 = ten2.shape[0]\n",
    "        \n",
    "        features_length = ten1.shape[1]\n",
    "        \n",
    "        # goes from [batch_size_1 x features_length] to [batch_size_1 x batch_size_2 x features_length]\n",
    "        expanded_1 = ten1.unsqueeze(1).expand(batch_size_1, batch_size_2, features_length)\n",
    "        expanded_2 = ten2.unsqueeze(0).expand(batch_size_1, batch_size_2, features_length)\n",
    "        \n",
    "        # sum over the dimension 2 collases features\n",
    "        # goes from [batch_size_1 x batch_size_2 x features_length] to [batch_size_1 x batch_size_2]\n",
    "        squared_distance = ((expanded_1 - expanded_2)**2).sum(2)\n",
    "        return squared_distance\n",
    "    \n",
    "    def extract_classwise_information(\n",
    "        self,\n",
    "        distance: torch.Tensor, # shape: [batch_size x batch_size]\n",
    "        samples_for_class: List[int], # shape: [num_classes]\n",
    "    ) -> List[torch.Tensor]:\n",
    "        # the idea is to create one square submatrix for each class with side\n",
    "        # equal to the amount of samples for that class inside the batch. In this way the\n",
    "        # size of matrices will be proportional to the impact of it inside the batch_size\n",
    "        square_submatrices_length = [length for length in samples_for_class]\n",
    "        \n",
    "        classes_distance = []\n",
    "        x_offset = 0\n",
    "        y_offset = 0\n",
    "        for length in square_submatrices_length:\n",
    "            class_distance = distance[x_offset:x_offset+length, y_offset:y_offset+length]\n",
    "            classes_distance.append(class_distance)\n",
    "            # translation over the diagonal of the matrix\n",
    "            x_offset += length\n",
    "            y_offset += length\n",
    "        return classes_distance\n",
    "    \n",
    "    def pairwise_class_mean_distance(\n",
    "        self,\n",
    "        samples_for_class_source: List[int], # shape: [num_classes] \n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "        distances: Dict[str, torch.Tensor] # shape [batch_size_1 x batch_size_2 x features_length]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # patches will contain the square submatrices computed in extract_classwise_information\n",
    "        patches = {}\n",
    "        mean_distances = {}\n",
    "        mean_distances['st'] = torch.zeros_like(distances['st'], requires_grad=False).to(self.device)\n",
    "        mean_distances['ss'] = []\n",
    "        mean_distances['tt'] = []\n",
    "        for c in range(len(samples_for_class_source)):\n",
    "            mean_distances['ss'] += [\n",
    "                torch.zeros(\n",
    "                    len(samples_for_class_source), requires_grad=False\n",
    "                ).to(self.device)\n",
    "            ]\n",
    "            mean_distances['tt'] += [\n",
    "                torch.zeros(\n",
    "                    len(samples_for_class_source), requires_grad=False\n",
    "                ).to(self.device)\n",
    "            ]\n",
    "        \n",
    "        classes_distance = []\n",
    "        source_start = 0\n",
    "        source_end = 0\n",
    "        for source_class_index in range(len(samples_for_class_source)):\n",
    "            source_stard = source_end\n",
    "            source_end = source_stard + samples_for_class_source[source_class_index]\n",
    "            patches['ss'] = distances['ss'][source_class_index]\n",
    "            \n",
    "            target_start = 0\n",
    "            target_end = 0\n",
    "            for target_class_index in range(len(samples_for_class_target)):\n",
    "                target_start = target_end\n",
    "                target_end = target_start + samples_for_class_target[target_class_index]\n",
    "                patches['tt'] = distances['tt'][target_class_index]\n",
    "                \n",
    "                patches['st'] = distances['st'][\n",
    "                    source_start:source_start + samples_for_class_source[source_class_index],\n",
    "                    target_start:target_start + samples_for_class_target[target_class_index]\n",
    "                ]\n",
    "                \n",
    "                mean = self.mean_estimation(patches)\n",
    "                \n",
    "                mean_distances['ss'][source_class_index][target_class_index] = mean\n",
    "                mean_distances['tt'][source_class_index][target_class_index] = mean\n",
    "                mean_distances['st'][source_stard:source_end, target_start:target_end] = mean\n",
    "\n",
    "        return mean_distances\n",
    "    \n",
    "    def mean_estimation(\n",
    "        self,\n",
    "        distances_patches: Dict[str, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        patches_sum = torch.sum(distances_patches['ss']) + \\\n",
    "            torch.sum(distances_patches['tt']) + \\\n",
    "            torch.sum(distances_patches['st'])\n",
    "        \n",
    "        batch_size_source = distances_patches['ss'].shape[0]\n",
    "        batch_size_target = distances_patches['tt'].shape[0]\n",
    "        \n",
    "        # batch_size_source and batch_size_taret are removed in order\n",
    "        # to obtain an unbiased estimator since total_amount_values\n",
    "        # will be used as denominator for the mean computation\n",
    "        total_amount_values = (batch_size_source * batch_size_source) + \\\n",
    "            (batch_size_target * batch_size_target) + \\\n",
    "            2 * (batch_size_source * batch_size_target) - \\\n",
    "            batch_size_source - batch_size_target\n",
    "        \n",
    "        mean = patches_sum.item() / total_amount_values\n",
    "        return mean\n",
    "    \n",
    "    def compute_cum_kernel_dist(\n",
    "        self,\n",
    "        layer_dist_st: List[torch.Tensor], # shape: [batch_size_source X batch_size_target]\n",
    "        layer_mean_st: List[torch.Tensor], # shape: [batch_size_source X batch_size_target]\n",
    "        class_index: Optional[int]=None\n",
    "    ):\n",
    "        cum_kernel_dist = None\n",
    "        for i, order, multiplier in zip(\n",
    "            range(self.num_layers),\n",
    "            self.kernel_orders,\n",
    "            self.kernel_multipliers\n",
    "        ):\n",
    "            distance = layer_dist_st[i] if class_index is None else layer_dist_st[i][class_index]\n",
    "            mean = layer_mean_st[i] if class_index is None else layer_mean_st[i][class_index]\n",
    "            \n",
    "            kernel_dist = self.compute_kernel_dist(distance, mean, order, multiplier)\n",
    "            if cum_kernel_dist is None:\n",
    "                cum_kernel_dist = kernel_dist\n",
    "            else:\n",
    "                cum_kernel_dist += kernel_dist\n",
    "                \n",
    "        return cum_kernel_dist\n",
    "    \n",
    "    # using the same kernel function from the original source code\n",
    "    # https://github.com/kgl-prml/Contrastive-Adaptation-Network-for-Unsupervised-Domain-Adaptation/blob/39acc516231a35206eb6277e527ee08acfdd07a6/discrepancy/cdd.py#L73\n",
    "    def compute_kernel_dist(\n",
    "        self,\n",
    "        distance: torch.Tensor, # shape: [batch_size_source X batch_size_target]\n",
    "        mean: torch.Tensor, # shape: [batch_size_source X batch_size_target]\n",
    "        kernel_order: int,\n",
    "        kernel_multiplier: int\n",
    "    ):\n",
    "        base_mean = mean / (kernel_multiplier ** (kernel_order // 2))\n",
    "        mean_list = [base_mean * (kernel_multiplier**i) for i in range(kernel_order)]\n",
    "        mean_tensor = torch.stack(mean_list, dim=0).to(self.device)\n",
    "\n",
    "        eps = 1e-5\n",
    "        mean_mask = (mean_tensor < eps).type(torch.FloatTensor)\n",
    "        mean_tensor = (1.0 - mean_mask) * mean_tensor + mean_mask * eps \n",
    "        mean_tensor = mean_tensor.detach()\n",
    "\n",
    "        for i in range(len(mean_tensor.size()) - len(distance.size())):\n",
    "            distance = distance.unsqueeze(0)\n",
    "\n",
    "        distance = distance / mean_tensor\n",
    "        upper_mask = (distance > 1e5).type(torch.FloatTensor).detach()\n",
    "        lower_mask = (distance < 1e-5).type(torch.FloatTensor).detach()\n",
    "        normal_mask = 1.0 - upper_mask - lower_mask\n",
    "        distance = normal_mask * distance + upper_mask * 1e5 + lower_mask * 1e-5\n",
    "        kernel_val = torch.sum(torch.exp(-1.0 * distance), dim=0)\n",
    "        return kernel_val\n",
    "    \n",
    "    def classes_mean_kernel(\n",
    "        self,\n",
    "        samples_for_class_source: List[int], # shape: [num_classes] \n",
    "        samples_for_class_target: List[int], # shape: [num_classes] \n",
    "        cumulate_kernel: torch.Tensor # shape: [batch_size_source X batch_size_target]\n",
    "    ) -> torch.Tensor: # shape: [num_classes X num_classes]\n",
    "        mean = torch.zeros(\n",
    "            (len(samples_for_class_source), len(samples_for_class_source))\n",
    "        ).to(self.device)\n",
    "        x_start = 0\n",
    "        x_end = 0\n",
    "        for source_class_index in range(len(samples_for_class_source)):\n",
    "            x_start = x_end\n",
    "            x_end = x_start + samples_for_class_source[source_class_index]\n",
    "            \n",
    "            y_start = 0\n",
    "            y_end = 0\n",
    "            for target_class_index in range(len(samples_for_class_target)):\n",
    "                y_start = y_end\n",
    "                y_end = y_start + samples_for_class_target[target_class_index]\n",
    "                \n",
    "                class_mean = torch.mean(\n",
    "                    cumulate_kernel[\n",
    "                        x_start:x_start + samples_for_class_source[source_class_index],\n",
    "                        y_start:y_start + samples_for_class_target[target_class_index]\n",
    "                    ]\n",
    "                )\n",
    "                mean[source_class_index, target_class_index] = class_mean\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b51f8-ce09-4bbc-9847-58ced589afcb",
   "metadata": {},
   "source": [
    "## Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79d94a-bdf3-4014-85cc-59d5340ac11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class SingleDataSourceDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_path: str,\n",
    "        name: str, \n",
    "    ):\n",
    "        self.inner_dataset = ImageFolder(\n",
    "            dataset_path,\n",
    "            transform=T.Compose([\n",
    "                T.Resize((256, 256)),\n",
    "                T.CenterCrop((224, 224)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        )\n",
    "        if name not in [\"source\", \"target\"]:\n",
    "            raise ValueError(\"name must be source or target\")\n",
    "        self.class_to_idx = self.inner_dataset.class_to_idx\n",
    "        self.class_indexes = np.array([class_id for _, class_id in self.inner_dataset.samples])\n",
    "        self.num_classes = len(self.inner_dataset.classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inner_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inner_dataset[idx]\n",
    "         \n",
    "class CANDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_dataset_path: str,\n",
    "        target_dataset_path: str,\n",
    "        samples_per_class: int,\n",
    "        classes_per_batch: int \n",
    "    ):\n",
    "        self.source_dataset = SingleDataSourceDataset(source_dataset_path, \"source\")\n",
    "        self.target_dataset = SingleDataSourceDataset(target_dataset_path, \"target\")\n",
    "        assert len(self.source_dataset) == len(self.target_dataset), \"Number of classes must be the same for source and target dataset\"\n",
    "        \n",
    "        self.num_classes = self.source_dataset.num_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.classes_per_batch = classes_per_batch\n",
    "        \n",
    "        self.source_class_indexes = [\n",
    "            np.argwhere(\n",
    "                self.source_dataset.class_indexes == idx\n",
    "            ).squeeze() for idx in range(self.num_classes)\n",
    "        ]\n",
    "        self.target_class_indexes = [\n",
    "            np.argwhere(\n",
    "                self.target_dataset.class_indexes == idx\n",
    "            ).squeeze() for idx in range(self.num_classes)\n",
    "        ]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_classes\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_idxs = np.random.choice(\n",
    "            self.source_class_indexes[idx],\n",
    "            size=self.samples_per_class, \n",
    "            replace=True\n",
    "        )\n",
    "        target_idxs = np.random.choice(\n",
    "            self.target_class_indexes[idx],            \n",
    "            size=self.samples_per_class, \n",
    "            replace=True\n",
    "        )\n",
    "        source_samples = [self.source_dataset[i] for i in source_idxs]\n",
    "        source_xs = torch.tensor([sample[0].numpy() for sample in source_samples])\n",
    "        source_ys = torch.tensor([sample[1] for sample in source_samples])\n",
    "        target_xs = torch.tensor([self.target_dataset[i][0].numpy() for i in target_idxs])\n",
    "        \n",
    "        return {\n",
    "            'source_xs': source_xs,\n",
    "            'source_ys': source_ys,\n",
    "            'target_xs': target_xs\n",
    "        }\n",
    "    \n",
    "def CAN_collate_function(data):\n",
    "    # if return data the shape will be:\n",
    "    # list of 4 array of 2 element (x,y) of 8 elements\n",
    "    return {\n",
    "        'source_xs': torch.cat([c['source_xs'] for c in data]),\n",
    "        'source_ys': torch.cat([c['source_ys'] for c in data]),\n",
    "        'target_xs': torch.cat([c['target_xs'] for c in data])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029bb6b-9766-45c5-8ac9-b412aaadc2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = DATASET_PATH / \"product_images\"\n",
    "target_path = DATASET_PATH / \"real_life\"\n",
    "batch_size = 32\n",
    "classes_per_batch = 4\n",
    "clustering_steps = 10\n",
    "\n",
    "source_dataset = SingleDataSourceDataset(source_path, \"source\")\n",
    "target_dataset = SingleDataSourceDataset(target_path, \"target\")\n",
    "can_dataset = CANDataset(\n",
    "    source_path,\n",
    "    target_path,\n",
    "    batch_size // classes_per_batch,\n",
    "    classes_per_batch\n",
    ")\n",
    "\n",
    "\n",
    "dataloaders = {\n",
    "    'source_net': DataLoader(\n",
    "        source_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    ),\n",
    "    'can': DataLoader(\n",
    "        can_dataset,\n",
    "        batch_size=classes_per_batch,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        collate_fn=CAN_collate_function\n",
    "    ),\n",
    "}\n",
    "\n",
    "clustering = {\n",
    "    'source': SingleDataSourceDataset(source_path, \"source\"),\n",
    "    'target': SingleDataSourceDataset(target_path, \"target\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c7d8c-dba2-4e47-9cb2-bccef76ef16c",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5377a71-503a-403e-bfe4-29f322145f87",
   "metadata": {},
   "source": [
    "To be able to compute the Contrastive Domain Discrepancy (CCD), it is necessary to know the ground truth for both the source and target domains: for this reason, a clustering technique is needed to estimate target labels using the features extracted by the network on the target dataset. The paper proposes spherical kmeans for this task, which uses the *cosine distance* metric:\n",
    "\n",
    "\\begin{equation}\n",
    "dist(a, b) = \\frac{1}{2}(1 - \\frac{\\langle a, b \\rangle}{\\|a\\| \\|b\\|})\n",
    "\\end{equation}\n",
    "\n",
    "Additionally, as reported in the paper, the number of cluster should be the number of classes in the source domain (which in this case is the same with respect to the target domain). Target centers should be initialized as the centers of the source domain.\n",
    "\n",
    "During the iterative step of the spherical kmeans algorithm, the update of the target center of class *c* is performed as:\n",
    "\n",
    "\\begin{equation}\n",
    "O^{tc} = \\sum_{i=1}^{N_t}\\textbf{1}_{\\hat{y}_i^t=c} \\frac{\\phi_l (x_i^t)}{\\| \\phi_l (x_i^t)\\|}\n",
    "\\end{equation}\n",
    "\n",
    "according to\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{1}_{\\hat{y}_i^t=c} = \n",
    "    \\begin{cases}\n",
    "      1 & \\textrm{if } \\hat{y}_i^t=c \\\\\n",
    "      0 & \\textrm{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "The iterative procedure is stopped when the mean distance of the centers computed by two adjacent iterations is below a certain threshold.\n",
    "\n",
    "An additional step is performed to avoid ambiguous label assignments:\n",
    "- each sample should have a distance to its assigned cluster below a certain threshold;\n",
    "- each cluster should have assigned at least a certain amount of samples.\n",
    "\n",
    "While in a first moment many classes could not be included, as the training proceeds the situation improves, since:\n",
    "- the accuracy of the model increases;\n",
    "- thanks to CCD, the intraclass domain discrepancy decreases and the interclass domain discrepancy increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cbbcf-c1aa-4ddd-b6ee-872f0ed1e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize, one_hot\n",
    "\n",
    "class Clustering:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        num_classes: int,\n",
    "        stop_threshold: float,\n",
    "        device: torch.device\n",
    "    ):\n",
    "        self.stop_threshold = stop_threshold\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.centers = None\n",
    "        self.device = device\n",
    "    \n",
    "    def set_init_centers(\n",
    "        self,\n",
    "        features: torch.tensor, # shape: [num_samples X num_features]\n",
    "        labels: torch.tensor # shape: [num_samples]\n",
    "    ):\n",
    "        self.init_centers = self.compute_centers(features, labels)\n",
    "        self.centers = self.init_centers\n",
    "        \n",
    "    def compute_centers(\n",
    "        self,\n",
    "        features: torch.tensor, # shape: [num_samples X num_features]\n",
    "        labels: torch.tensor # shape: [num_samples]\n",
    "    ):\n",
    "        centers = torch.zeros((self.num_classes, features.size(1))).to(self.device)\n",
    "        for class_id in range(self.num_classes):\n",
    "            class_sample_positions = (labels == class_id)\n",
    "            centers[class_id] = torch.sum(features[class_sample_positions], dim=0)\n",
    "        return centers\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        features: torch.tensor, # shape: [num_samples X num_features]\n",
    "    ) -> torch.tensor: # shape: [num_samples]\n",
    "        assert self.init_centers is not None, \"initialize the centers of clusters\"\n",
    "        while True:\n",
    "            centers = torch.zeros((self.num_classes, features.size(1))).to(self.device)\n",
    "            class_counts = torch.zeros(self.num_classes).to(self.device)\n",
    "            for start in range(0, features.size(0), self.batch_size):\n",
    "                end = min(start + self.batch_size, features.size(0))\n",
    "                batch = features[start:end]\n",
    "                _, labels = self.classify_samples(batch)\n",
    "                # labels_onehot shape: [batch_size X num_classes]\n",
    "                labels_onehot = one_hot(labels, self.num_classes)\n",
    "\n",
    "                class_counts += torch.sum(labels_onehot, dim=0)\n",
    "                centers += self.compute_centers(batch, labels)\n",
    "\n",
    "            # checks if one class has not any sample, in this way no update on those clusters\n",
    "            mask = (count.unsqueeze(1) > 0).float().to(self.device)\n",
    "            centers = mask * centers + (1 - mask) * self.init_centers\n",
    "            if self.check_stop_criterion(centers):\n",
    "                break\n",
    "            self.centers = centers\n",
    "        self.centers = centers\n",
    "        \n",
    "        # TODO: see if the alignment is a fundamental part\n",
    "        \n",
    "        _, labels = self.classify_samples(features)\n",
    "        return labels\n",
    "        \n",
    "        \n",
    "    def classify_samples(\n",
    "        self,\n",
    "        features: torch.tensor # shape: [batch size X num_features]\n",
    "    )-> Tuple[torch.tensor, torch.tensor]:\n",
    "        distances = self.compute_cosine_distance(\n",
    "            features,\n",
    "            self.centers,\n",
    "            cross=True\n",
    "        )\n",
    "        _, labels = torch.min(distances, dim=1)\n",
    "        # TODO: understand why distances are returned instead of _\n",
    "        return distances, labels\n",
    "    \n",
    "    def compute_cosine_distance(\n",
    "        self,\n",
    "        tensor_1: torch.tensor,\n",
    "        tensor_2: torch.tensor,\n",
    "        cross: bool\n",
    "    ) -> torch.tensor:\n",
    "        tensor_1 = normalize(tensor_1, dim=1)\n",
    "        tensor_2 = normalize(tensor_2, dim=1)\n",
    "        if not cross:\n",
    "            return 0.5 * (1.0 - torch.sum(tensor_1 * tensor_2, dim=1))\n",
    "        else:\n",
    "            assert(tensor_1.size(1) == tensor_2.size(1))\n",
    "            return 0.5 * (1.0 - torch.matmul(tensor_1, tensor_2.transpose(0, 1)))\n",
    "\n",
    "    def check_stop_criterion(self, centers):\n",
    "        distances = self.compute_cosine_distance(centers, self.centers, cross=False)\n",
    "        distances = torch.mean(distances, dim=0)\n",
    "        print('Clustering distance %.8f' % distances.item())\n",
    "        return distances.item() < self.stop_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c776624-25af-47d0-a932-a8fd2027d310",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055f7b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef2e95c",
   "metadata": {},
   "source": [
    "All models are composed by three main elements:\n",
    "\n",
    "- backbone\n",
    "- feature processor\n",
    "- classifier\n",
    "\n",
    "The backbone is a pretrained network from `torchvision` trained with a smaller learning rate than the one used for other two components.\n",
    "The feature extractor is used for process the output of the backbone, we decided to separate these two steps into two classes in order to have more control on both of them, this allow to make tests with many combinations.\n",
    "The last element is the classifier, it is a simple linear layer that takes the output of backbone (feature processor if it is used) and return an array of prediction with the same length of the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb394a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ac533",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Backbone(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_type: str,\n",
    "    ) -> None:\n",
    "        super(Backbone, self).__init__()\n",
    "        if backbone_type == \"resnet34\":\n",
    "            self.backbone = models.resnet34(pretrained=True)\n",
    "            self.output_num = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Sequential()\n",
    "        else:\n",
    "            raise ValueError(\"select one valid backbone_type\")\n",
    "            \n",
    "        \n",
    "    def get_output_num(self):\n",
    "        return self.output_num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "    \n",
    "class FeatureProcessor(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        super(FeatureProcessor, self).__init__()\n",
    "        self.processing_layer = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_num = output_size\n",
    "        \n",
    "    def get_output_num(self):\n",
    "        return self.output_num\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.processing_layer(x)\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_type: str,\n",
    "        n_classes: int,\n",
    "        n_feature_extracted: Optional[int] = None\n",
    "    ) -> None:\n",
    "        super(Classifier, self).__init__()\n",
    "        self.backbone = Backbone(backbone_type)\n",
    "        if n_feature_extracted:\n",
    "            self.processing_features = True\n",
    "            self.processing_layer = FeatureProcessor(self.backbone.get_output_num(), n_feature_extracted)\n",
    "        else:\n",
    "            self.processing_features = False\n",
    "        self.classification_layer = nn.Linear(\n",
    "            self.processing_layer.get_output_num() if self.processing_features else self.backbone.get_output_num(),\n",
    "            n_classes\n",
    "        )\n",
    "        self.output_num = n_classes\n",
    "        \n",
    "    def get_output_num(self):\n",
    "        self.output_num\n",
    "        \n",
    "    def get_features(self, x):\n",
    "        features = self.backbone(x)\n",
    "        if self.processing_features:\n",
    "            features = self.processing_layer(features)\n",
    "        return features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.get_features(x)\n",
    "        classes = self.classification_layer(features)\n",
    "        \n",
    "        return features, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e68f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181e059",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc7459",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "source_dataset = ImageFolder(\n",
    "    DATASET_PATH / \"product_images\",\n",
    "    transform=T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "target_dataset = ImageFolder(\n",
    "    DATASET_PATH / \"real_life\",\n",
    "    transform=T.Compose([\n",
    "        T.Resize((256, 256)),\n",
    "        T.CenterCrop((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    ")\n",
    "\n",
    "validation_ratio = .2\n",
    "test_ratio = .2\n",
    "\n",
    "validation_dataset, test_dataset, train_dataset = random_split(\n",
    "    source_dataset,\n",
    "    [\n",
    "        int(len(source_dataset)*validation_ratio),\n",
    "        int(len(source_dataset)*test_ratio),\n",
    "        len(source_dataset) - (\n",
    "            int(len(source_dataset)*validation_ratio)\n",
    "            +int(len(source_dataset)*test_ratio)\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_test_ratio = .4\n",
    "target_test_dataset, target_train_dataset = random_split(\n",
    "    target_dataset,\n",
    "    [\n",
    "        int(len(target_dataset)*target_test_ratio),\n",
    "        len(target_dataset) - int(len(target_dataset)*target_test_ratio)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317c8ee9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(train_dataset), len(validation_dataset), len(test_dataset), len(target_train_dataset), len(target_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0606c41f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf5a20",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler, SGD, Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "#import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb2b01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# misc\n",
    "set_random_seed(33)\n",
    "device = get_device()\n",
    "num_threads = 4\n",
    "\n",
    "# train\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "weight_decay = 0.000001\n",
    "momentum = 0.9\n",
    "scheduler_factor = 0.25\n",
    "scheduler_patience = 20\n",
    "\n",
    "# domain adaptation loss\n",
    "use_coral = True\n",
    "coral_weight = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd88f0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, num_threads):\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_threads\n",
    "    )\n",
    "\n",
    "def get_optimizer(model, lr, wd, momentum):\n",
    "    higher_lr_weights = []\n",
    "    lower_lr_weights = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith('backbone'):\n",
    "            lower_lr_weights.append(param)\n",
    "        elif name.startswith('processing') or name.startswith('classification'):\n",
    "            higher_lr_weights.append(param)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected type of weights {name}\")\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [\n",
    "            {'params': lower_lr_weights},\n",
    "            {'params': higher_lr_weights, 'lr': lr}\n",
    "        ],\n",
    "        lr=lr / 10,\n",
    "        weight_decay=wd,\n",
    "        momentum=momentum\n",
    "    )\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063aa8d7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = Classifier(\n",
    "    backbone_type=\"resnet34\", \n",
    "    n_classes=len(dataset.classes),\n",
    "    # n_feature_extracted=2048\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = get_optimizer(model, lr, weight_decay, momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    'min',\n",
    "    factor=scheduler_factor,\n",
    "    patience=scheduler_patience,\n",
    "    verbose=True\n",
    ") \n",
    "dataloaders = {\n",
    "    'train': {\n",
    "        'source': get_data_loader(train_dataset, batch_size, num_threads),\n",
    "        'target': get_data_loader(target_train_dataset, batch_size, num_threads),\n",
    "    },\n",
    "    'validation': {\n",
    "        'source': get_data_loader(validation_dataset, batch_size, num_threads),\n",
    "    }\n",
    "    #'test': get_data_loader(test_dataset, batch_size, num_threads)\n",
    "}\n",
    "#summary(model, input_size=(batch_size, *dataset[0][0].size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319f0bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = model\n",
    "best_loss = np.Inf\n",
    "\n",
    "phases_loss = {k: [] for k in dataloaders.keys()}\n",
    "phases_acc = {k: [] for k in dataloaders.keys()}\n",
    "for epoch in range(num_epochs):\n",
    "    for phase, phase_dataloaders in dataloaders.items():\n",
    "        source_dataloader = phase_dataloaders['source']\n",
    "        target_dataloader = phase_dataloaders.get('target', None)\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        for index, (x, labels) in enumerate(source_dataloader):\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type=device.type):\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    features, predictions = model(x)\n",
    "                    x = x.detach().cpu()\n",
    "                    \n",
    "                    loss = criterion(predictions, labels)\n",
    "                    predictions = predictions.detach().cpu()\n",
    "                    labels = labels.detach().cpu()\n",
    "                    if use_coral and not target_dataloader is None:\n",
    "                        source_feauteres = features.cpu()\n",
    "                        target_x, _ = next(target_dataloader._get_iterator())\n",
    "                        target_x = target_x.to(device)\n",
    "                        target_fuatures, _ = model(target_x)\n",
    "                        target_x = target_x.detach().cpu()\n",
    "                        target_fuatures = target_fuatures.cpu()\n",
    "                        loss = loss + coral_weight * coral(source_feauteres, target_fuatures)\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += (torch.argmax(predictions, 1) == labels).sum().item() / labels.size()[0]\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "        epoch_loss /= len(source_dataloader)\n",
    "        epoch_acc /= len(source_dataloader)\n",
    "        phases_loss[phase].append(epoch_loss)\n",
    "        phases_acc[phase].append(epoch_acc)\n",
    "\n",
    "        if phase == \"validation\" and abs(epoch_loss) <= abs(best_loss):\n",
    "            best_model = model\n",
    "            best_loss = epoch_loss\n",
    "            scheduler.step(epoch_loss)\n",
    "            \n",
    "    print_epoch_chart(\n",
    "        phases_loss['train'],\n",
    "        phases_loss['validation'],\n",
    "        phases_acc['train'],\n",
    "        phases_acc['validation'],\n",
    "        epoch,\n",
    "        num_epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1261a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7ff39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "predictions = []\n",
    "reals = []\n",
    "for x, y in test_dataset:\n",
    "    x_gpu = x.unsqueeze(0).to(device)\n",
    "    feature, predicted = best_model(x_gpu)\n",
    "    features.append(feature.detach().cpu().numpy())\n",
    "    predictions.append(np.argmax(predicted.detach().cpu().numpy()))\n",
    "    reals.append(y)\n",
    "    del x_gpu\n",
    "features = np.array(features)\n",
    "predictions = np.array(predictions)\n",
    "reals = np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3744a6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(index, idx_to_class, dataset, reals, predictions):\n",
    "    x, _ = dataset[index]\n",
    "    plt.imshow(x[1])\n",
    "    print(idx_to_class[reals[index]], idx_to_class[predictions[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11000b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictions(350, idx_to_class, test_dataset, reals, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c74b3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(reals == predictions).sum() / reals.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30b96d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a15c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6caa3c9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "predictions = []\n",
    "reals = []\n",
    "for x, y in target_dataset:\n",
    "    x_gpu = x.unsqueeze(0).to(device)\n",
    "    feature, predicted = best_model(x_gpu)\n",
    "    features.append(feature.detach().cpu().numpy())\n",
    "    predictions.append(np.argmax(predicted.detach().cpu().numpy()))\n",
    "    reals.append(y)\n",
    "    del x_gpu\n",
    "features = np.array(features)\n",
    "predictions = np.array(predictions)\n",
    "reals = np.array(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5032554",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_predictions(index, idx_to_class, dataset, reals, predictions):\n",
    "    x, _ = dataset[index]\n",
    "    plt.imshow(x[1])\n",
    "    print(idx_to_class[reals[index]], idx_to_class[predictions[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f47c70",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictions(350, idx_to_class, target_dataset, reals, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e706e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(reals == predictions).sum() / reals.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97426ae3-61a3-48ee-871f-f6888bbe092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP()\n",
    "embedding = reducer.fit_transform(np.squeeze(features, 1))\n",
    "\n",
    "plt.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=reals,\n",
    "    s=5\n",
    ")\n",
    "plt.title('UMAP projection of features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff04a18-665f-459a-9a68-64914d2405ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "name": "project.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
